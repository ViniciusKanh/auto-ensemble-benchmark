# üì¶ Auto-Ensemble-Benchmark  
### üî• Benchmark autom√°tico de modelos *ensemble* com m√©tricas completas, ranking e valida√ß√£o cruzada

<p align="center">
  <img src="https://img.shields.io/badge/status-0.1.0-4ade80?style=for-the-badge" />
  <img src="https://img.shields.io/badge/python-3.9+-3776ab?style=for-the-badge&logo=python&logoColor=white" />
  <img src="https://img.shields.io/badge/scikit--learn-ML-F7931E?style=for-the-badge&logo=scikitlearn&logoColor=white" />
  <img src="https://img.shields.io/badge/license-MIT-22c55e?style=for-the-badge" />
</p>

---

## üåü Vis√£o Geral

`auto-ensemble-benchmark` √© uma biblioteca em Python para **comparar automaticamente m√∫ltiplos modelos ensemble de classifica√ß√£o**, calculando m√©tricas padronizadas e organizando os resultados em **tabelas claras, ordenadas e interpret√°veis**.

Ela foi pensada para:

- üí° **Pesquisadores** que precisam comparar rapidamente ensembles em diferentes bases.  
- üß™ **Data Scientists** que querem um benchmark r√°pido de modelos baselines.  
- üìä **Estudos acad√™micos** que requerem m√©tricas reprodut√≠veis e relat√≥rios consistentes.  

Com poucas linhas de c√≥digo, voc√™ consegue:

- Treinar v√°rios ensembles de uma vez (RandomForest, ExtraTrees, GradientBoosting, AdaBoost, Bagging com RF).  
- Obter uma tabela com:

  - `accuracy`  
  - `f1`  
  - `recall`  
  - `precision`  
  - Ranking por m√©trica (`rank_<m√©trica>`)  
  - Ranking global (`overall_rank`)

- Rodar **valida√ß√£o cruzada (k-fold)** e obter m√©dia/desvio-padr√£o das m√©tricas.  
- Salvar tudo em **CSV**, organizado por experimento.

---

## üß© Principais Recursos

- üîÅ **Ensembles prontos para uso**
  - `RandomForestClassifier`
  - `ExtraTreesClassifier`
  - `GradientBoostingClassifier`
  - `AdaBoostClassifier`
  - `BaggingClassifier` com base em RandomForest

- üìä **M√©tricas j√° calculadas**
  - `accuracy`
  - `f1`
  - `recall`
  - `precision`

- üèÜ **Ranking autom√°tico**
  - Ranking por m√©trica: `rank_accuracy`, `rank_f1`, etc.
  - Ranking global: `overall_rank` (soma das posi√ß√µes, menor = melhor).

- üéØ **M√©trica principal configur√°vel**
  - `primary_metric="accuracy"` (padr√£o) ou `"f1"`, `"recall"`, `"precision"`.

- üîß **Customiza√ß√£o de hiperpar√¢metros**
  - Via `model_overrides={"RandomForest": {"n_estimators": 500}, ...}` sem precisar recriar tudo na m√£o.

- üìÇ **Persist√™ncia de resultados**
  - Define pasta (`output_dir`) e nome de experimento (`experiment_name`).
  - Salva automaticamente:
    - Resultados de hold-out
    - Resultados de valida√ß√£o cruzada
  - Formato CSV, pronto para an√°lise posterior / relat√≥rios.

- üß™ **Valida√ß√£o cruzada integrada**
  - `fit_evaluate_cv(X, y, cv=5, ...)`  
  - Retorna m√©tricas `_mean` e `_std` para cada modelo.

---

## üì¶ Instala√ß√£o

### 1Ô∏è‚É£ Clonar o reposit√≥rio

```bash
git clone https://github.com/ViniciusKanh/auto-ensemble-benchmark.git
cd auto-ensemble-benchmark
````

### 2Ô∏è‚É£ Criar e ativar ambiente virtual (recomendado)

```bash
python -m venv .venv

# Windows
.venv\Scripts\activate

# Linux / macOS
source .venv/bin/activate
```

### 3Ô∏è‚É£ Instalar depend√™ncias em modo desenvolvimento

```bash
pip install -e .
```

Isso ir√°:

* Instalar o pacote `auto-ensemble-benchmark` em modo edit√°vel.
* Permitir que voc√™ edite o c√≥digo e teste sem reinstalar.

> üí° *Quando (e se) houver publica√ß√£o no PyPI, a instala√ß√£o ficar√° t√£o simples quanto:*
> `pip install auto-ensemble-benchmark`

---

## üöÄ Exemplo R√°pido (Wine Dataset)

Uso m√≠nimo da biblioteca com o dataset cl√°ssico `wine` do scikit-learn:

```python
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split

from auto_ensemble_benchmark import AutoEnsembleClassifier

# 1. Dados
data = load_wine()
X = data.data
y = data.target

X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.3,
    random_state=42,
    stratify=y,
)

# 2. Instanciar o avaliador autom√°tico (modo default)
auto = AutoEnsembleClassifier()

# 3. Treinar + avaliar (hold-out)
resultados_holdout = auto.fit_evaluate(
    X_train,
    y_train,
    X_test,
    y_test,
    media="macro",  # m√©dia macro para problema multiclasse
)

print("Resultados (hold-out):")
print(resultados_holdout)

# 4. Resumo textual
print("\nResumo interpretado:")
print(auto.summarize_results(top_k=3))
```

Sa√≠da t√≠pica (resumida):

```text
Resultados (hold-out):
                          accuracy        f1    recall  precision  rank_accuracy  rank_f1  ...
modelo
RandomForest              1.000000  1.000000  1.000000   1.000000              1        1
ExtraTrees                1.000000  1.000000  1.000000   1.000000              1        1
Bagging_RandomForestBase  1.000000  1.000000  1.000000   1.000000              1        1
AdaBoost                  0.98...   ...
GradientBoosting          0.96...   ...

Resumo interpretado:
Melhor modelo (segundo accuracy): RandomForest com accuracy = 1.0000.
Top modelos (hold-out):
  1. RandomForest -> ...
  2. ExtraTrees -> ...
  3. Bagging_RandomForestBase -> ...
```

---

## ‚öôÔ∏è Uso com Par√¢metros Avan√ßados

Exemplo configurando:

* m√©trica principal (`primary_metric="f1"`)
* sobrescrita de hiperpar√¢metros (`model_overrides`)
* pasta de sa√≠da e nome de experimento
* salvamento autom√°tico em CSV

```python
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split

from auto_ensemble_benchmark import AutoEnsembleClassifier

data = load_wine()
X = data.data
y = data.target

X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.3,
    random_state=123,
    stratify=y,
)

auto = AutoEnsembleClassifier(
    metricas=["accuracy", "f1", "recall", "precision"],
    primary_metric="f1",           # ranking guiado por F1
    add_rank_columns=True,         # inclui rank_<m√©trica> e overall_rank
    model_overrides={
        "RandomForest": {"n_estimators": 500, "max_depth": None},
        "ExtraTrees": {"n_estimators": 800},
    },
    output_dir="resultados_bench",     # pasta para salvar CSVs
    experiment_name="wine_ensembles",  # prefixo dos arquivos
    save_on_evaluate=True,            # salva hold-out
    save_on_cv=True,                  # salva cross-validation
)

resultados_holdout = auto.fit_evaluate(
    X_train,
    y_train,
    X_test,
    y_test,
    media="macro",
)

print(resultados_holdout)

resultados_cv = auto.fit_evaluate_cv(
    X,
    y,
    cv=5,
    media="macro",
    random_state_cv=123,
    shuffle=True,
)

print(resultados_cv)
```

Arquivos gerados (exemplo):

* `resultados_bench/wine_ensembles_holdout_results.csv`
* `resultados_bench/wine_ensembles_cv_results.csv`

---

## üß™ Exemplo em Dataset Mais Dif√≠cil

Para testar a biblioteca em um cen√°rio mais desafiador, podemos usar `make_classification` com:

* classes desbalanceadas
* ru√≠do nos r√≥tulos (`flip_y`)
* separa√ß√£o moderada entre classes

```python
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report

from auto_ensemble_benchmark import AutoEnsembleClassifier

# Dataset sint√©tico dif√≠cil (bin√°rio)
X, y = make_classification(
    n_samples=2000,
    n_features=30,
    n_informative=10,
    n_redundant=5,
    n_clusters_per_class=2,
    weights=[0.7, 0.3],   # desbalanceado
    flip_y=0.05,          # 5% de ru√≠do de r√≥tulo
    class_sep=0.8,        # separa√ß√£o moderada
    random_state=42,
)

X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.3,
    random_state=42,
    stratify=y,
)

auto = AutoEnsembleClassifier(
    metricas=["accuracy", "f1", "recall", "precision"],
    primary_metric="f1",  # aqui o F1 faz mais sentido como m√©trica principal
    add_rank_columns=True,
)

resultados_holdout = auto.fit_evaluate(
    X_train,
    y_train,
    X_test,
    y_test,
    media="binary",  # problema bin√°rio
)

print("Resultados (hold-out) - dataset dif√≠cil:")
print(resultados_holdout)

print("\nResumo interpretado:")
print(auto.summarize_results(top_k=3))

# Inspecionar o melhor modelo em detalhes
best_model = auto.best_model_

if best_model is not None:
    y_pred = best_model.predict(X_test)

    print("\nMatriz de confus√£o (melhor modelo):")
    print(confusion_matrix(y_test, y_pred))

    print("\nClassification report (melhor modelo):")
    print(classification_report(y_test, y_pred, digits=4))
```

Aqui voc√™ ver√° m√©tricas mais realistas (por exemplo, accuracy ~0.85, f1 ~0.74) e diferen√ßas claras entre os ensembles.

---

## üìö API ‚Äì Refer√™ncia R√°pida

### `AutoEnsembleClassifier`

```python
from auto_ensemble_benchmark import AutoEnsembleClassifier
```

#### Construtor

```python
auto = AutoEnsembleClassifier(
    modelos=None,
    metricas=None,
    primary_metric="accuracy",
    add_rank_columns=True,
    random_state=42,
    n_jobs=-1,
    model_overrides=None,
    output_dir=None,
    experiment_name=None,
    save_on_evaluate=False,
    save_on_cv=False,
)
```

* `modelos` (`dict[str, estimator]`, opcional)
  Se `None`, usa os modelos padr√£o do `model_zoo` (RandomForest, ExtraTrees, etc).

* `metricas` (`list[str]`, opcional)
  Lista de m√©tricas a calcular. Suporta:
  `["accuracy", "f1", "recall", "precision"]`
  Se `None`, usa todas.

* `primary_metric` (`str`)
  M√©trica principal usada para:

  * ordenar a tabela de resultados
  * definir o ‚Äúmelhor modelo‚Äù (`best_model_` / `best_model_name_`)

* `add_rank_columns` (`bool`)
  Se `True`, adiciona colunas `rank_<m√©trica>` e `overall_rank`.

* `random_state` (`int`)
  Semente padr√£o usada na cria√ß√£o dos modelos default.

* `n_jobs` (`int`)
  N√∫mero de n√∫cleos para paraleliza√ß√£o (quando suportado pela implementa√ß√£o do modelo).

* `model_overrides` (`dict[str, dict]`, opcional)
  Sobrescreve hiperpar√¢metros dos modelos padr√µes. Exemplo:

  ```python
  model_overrides = {
      "RandomForest": {"n_estimators": 500, "max_depth": 10},
      "ExtraTrees": {"n_estimators": 1000},
  }
  ```

* `output_dir` (`str`, opcional)
  Caminho para pasta onde salvar CSVs com resultados. Se `None`, n√£o salva.

* `experiment_name` (`str`, opcional)
  Prefixo dos arquivos CSV. Default: `"auto_ensemble_experiment"`.

* `save_on_evaluate` (`bool`)
  Se `True`, salva resultados de hold-out ao final de `evaluate`/`fit_evaluate`.

* `save_on_cv` (`bool`)
  Se `True`, salva resultados de `fit_evaluate_cv`.

---

### M√©todos principais

#### `fit(X_train, y_train)`

Treina todos os modelos com os dados de treino.

```python
auto.fit(X_train, y_train)
```

#### `evaluate(X_test, y_test, media="auto")`

Avalia todos os modelos com dados de teste.

```python
df_resultados = auto.evaluate(X_test, y_test, media="macro")
```

* `media`:

  * `"auto"` ‚Üí detecta bin√°rio vs. multiclasse e escolhe `'binary'` ou `'macro'`.
  * ou qualquer valor aceito pelas m√©tricas do scikit-learn (`"binary"`, `"macro"`, `"micro"`, `"weighted"` etc.).

Retorna `DataFrame` com:

* colunas de m√©tricas (`accuracy`, `f1`, `recall`, `precision`).
* colunas de ranking (`rank_<m√©trica>`) se `add_rank_columns=True`.
* `overall_rank` (soma dos ranks).

#### `fit_evaluate(X_train, y_train, X_test, y_test, media="auto")`

Atalho: treina e avalia em uma √∫nica chamada.

```python
df_resultados = auto.fit_evaluate(
    X_train, y_train, X_test, y_test, media="macro"
)
```

#### `fit_evaluate_cv(X, y, cv=5, media="auto", random_state_cv=42, shuffle=True)`

Avalia√ß√£o com valida√ß√£o cruzada estratificada.

```python
df_cv = auto.fit_evaluate_cv(
    X, y,
    cv=5,
    media="macro",
    random_state_cv=42,
    shuffle=True,
)
```

Retorna `DataFrame` com:

* `<m√©trica>_mean`
* `<m√©trica>_std`

para cada modelo.

#### `get_results()`

Retorna o √∫ltimo `DataFrame` de resultados de hold-out.

```python
df_holdout = auto.get_results()
```

#### `get_results_cv()`

Retorna o √∫ltimo `DataFrame` de resultados de cross-validation.

```python
df_cv = auto.get_results_cv()
```

#### `summarize_results(top_k=3)`

Gera um resumo textual do desempenho dos modelos no hold-out.

```python
print(auto.summarize_results(top_k=3))
```

Exemplo de sa√≠da:

```text
Melhor modelo (segundo f1): GradientBoosting com f1 = 0.7692.

Top modelos (hold-out):
  1. GradientBoosting -> accuracy=0.8650, f1=0.7692, recall=0.7181, precision=0.8282, overall_rank=8.0
  2. RandomForest -> ...
  3. Bagging_RandomForestBase -> ...

Diferen√ßa entre o 1¬∫ e o 2¬∫ em f1: 0.0065.
```

---

## üõ£Ô∏è Roadmap (Ideias Futuras)

* ‚úÖ Vers√£o inicial para classifica√ß√£o com ensembles.
* ‚è≥ `AutoEnsembleRegressor` (vers√£o para regress√£o, com RMSE, MAE, R¬≤ etc.).
* ‚è≥ Integra√ß√£o com XGBoost, LightGBM e CatBoost.
* ‚è≥ Gera√ß√£o autom√°tica de relat√≥rio em Markdown / LaTeX para artigos.
* ‚è≥ Plots autom√°ticos (boxplots das m√©tricas, barras com intervalos de confian√ßa).
* ‚è≥ Publica√ß√£o no PyPI.

---

## ü§ù Contribuindo

Contribui√ß√µes s√£o bem-vindas:

1. Fa√ßa um fork do reposit√≥rio.
2. Crie uma branch para sua feature/fix:

   ```bash
   git checkout -b minha-feature
   ```
3. Fa√ßa commits claros.
4. Abra um Pull Request explicando:

   * o problema
   * a solu√ß√£o adotada
   * se poss√≠vel, inclua exemplos/prints das m√©tricas.

Sugest√µes de contribui√ß√µes:

* Novos modelos no `model_zoo` (ensembles adicionais).
* Testes unit√°rios com `pytest`.
* Melhorias no README / documenta√ß√£o.
* Integra√ß√£o com mais tipos de datasets e exemplos.

---

## üìÑ Licen√ßa

Este projeto √© distribu√≠do sob a licen√ßa **MIT**.
Voc√™ √© livre para usar, modificar e distribuir, desde que mantenha os cr√©ditos originais.

---

Claro ‚Äî segue um t√≥pico profissional, elegante, acad√™mico e bem-estruturado com **cr√©ditos completos**, **cart√£o profissional**, **foto**, **links**, mantendo o estilo formal e t√©cnico do restante do README.

Voc√™ s√≥ precisa **copiar e colar no final do seu README**:

---

## üë§ Autor ‚Äì Sobre o Pesquisador

### **Vinicius de Souza Santos**  
**Pesquisador em Ci√™ncia da Computa√ß√£o ‚Äì UNESP**  
**√änfase em Data Science, Machine Learning e Engenharia de Modelos**

<p align="left">
  <img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQPGe7oWAt4D6YLkx8btAGVGFCecJa1oXFzAA&s" width="180" style="border-radius:12px;"/>
</p>

---

### üßë‚Äçüéì Forma√ß√£o & Atua√ß√£o
- Mestrando em **Ci√™ncia da Computa√ß√£o** pela **UNESP**  
- Pesquisador com foco em:
  - Feature Selection  
  - Benchmarking de modelos  
  - M√©todos Ensemble  
  - Engenharia de Dados aplicada a Machine Learning  
- Engenheiro de Computa√ß√£o em forma√ß√£o  
- Experi√™ncia com Python, Scikit-Learn, Data Mining, Experimenta√ß√£o e valida√ß√£o de modelos

---

### üåê Redes e Contato

- **LinkedIn:**  
  üëâ https://www.linkedin.com/in/vinicius-souza-santoss/

- **GitHub:**  
  üëâ https://github.com/ViniciusKanh

- **E-mail acad√™mico:**  
  üëâ vinicius-souza.santos@unesp.br

---

### üí¨ Nota do Autor

Este projeto foi desenvolvido com a proposta de oferecer uma biblioteca simples, transparente e cient√≠fica para benchmark automatizado de modelos ensemble, focando em reprodutibilidade, rigor estat√≠stico e facilidade de uso para pesquisadores, estudantes e profissionais da √°rea.

---

