# üì¶ Auto-Ensemble-Benchmark

### üî¨ Benchmark autom√°tico de modelos *ensemble* para classifica√ß√£o ‚Äî m√©tricas padronizadas, ranking e valida√ß√£o cruzada

<p align="center">
  <img src="https://img.shields.io/badge/version-0.1.1-4ade80?style=for-the-badge" alt="version" />
  <img src="https://img.shields.io/badge/python-3.9%2B-3776ab?style=for-the-badge&logo=python&logoColor=white" alt="python" />
  <img src="https://img.shields.io/badge/license-MIT-22c55e?style=for-the-badge" alt="license" />
</p>

---

## Sum√°rio

1. [Vis√£o geral](#vis%C3%A3o-geral)
2. [Instala√ß√£o](#instala%C3%A7%C3%A3o)
3. [Exemplos de uso](#exemplos-de-uso)

   * [Exemplo r√°pido (hold-out)](#exemplo-r%C3%A1pido-hold-out)
   * [Valida√ß√£o cruzada (CV)](#valida%C3%A7%C3%A3o-cruzada-cv)
4. [Explica√ß√£o dos resultados e formato dos arquivos](#explica%C3%A7%C3%A3o-dos-resultados-e-formato-dos-arquivos)
5. [Interpreta√ß√£o metodol√≥gica e recomenda√ß√µes](#interpreta%C3%A7%C3%A3o-metodol%C3%B3gica-e-recomenda%C3%A7%C3%B5es)
6. [API ‚Äî refer√™ncia sucinta](#api---refer%C3%AAncia-sucinta)
7. [Roadmap](#roadmap)
8. [Contribui√ß√£o](#contribui%C3%A7%C3%A3o)
9. [Licen√ßa](#licen%C3%A7a)
10. [Sobre o autor](#sobre-o-autor)
11. [Changelog curto](#changelog-curto)

---

## Vis√£o geral

`auto-ensemble-benchmark` √© uma biblioteca Python projetada para automatizar a compara√ß√£o de classificadores *ensemble* por meio de m√©tricas padronizadas, rankings por m√©trica, ranking agregado e valida√ß√£o cruzada. O objetivo √© proporcionar um fluxo reprodut√≠vel e cient√≠fico para:

* Treinar e avaliar m√∫ltiplos ensembles baseline (RandomForest, ExtraTrees, GradientBoosting, AdaBoost, Bagging com RF).
* Calcular m√©tricas padr√£o (accuracy, f1, recall, precision) com suporte a m√©dias para problemas multiclasse/bin√°rio.
* Gerar colunas de ranking por m√©trica e um `overall_rank` agregador.
* Executar valida√ß√£o cruzada estratificada retornando m√©dia e desvio-padr√£o.
* Persistir resultados em CSV para documenta√ß√£o experimental e relat√≥rios cient√≠ficos.

Aplica√ß√µes t√≠picas: pesquisa acad√™mica (benchmarks reprodut√≠veis), avalia√ß√£o de baselines por cientistas de dados, provas de conceito r√°pidas em conjuntos de dados variados.

---

## Instala√ß√£o

### Via PyPI (recomendado)

```bash
pip install auto-ensemble-benchmark
```

### Modo desenvolvimento (instala√ß√£o local)

```bash
git clone https://github.com/ViniciusKanh/auto-ensemble-benchmark.git
cd auto-ensemble-benchmark

# criar ambiente virtual (exemplo)
python -m venv .venv

# Windows
.venv\Scripts\activate

# macOS / Linux
source .venv/bin/activate

pip install -e .
```

---

## Exemplos de uso

> Todos os exemplos assumem importa√ß√µes padr√£o do `scikit-learn`. Coment√°rios dos trechos de c√≥digo est√£o em Portugu√™s.

### Exemplo r√°pido ‚Äî hold-out

```python
# Exemplo m√≠nimo: hold-out com dataset wine
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
from auto_ensemble_benchmark import AutoEnsembleClassifier

# Carrega dados
data = load_wine()
X, y = data.data, data.target

# Particiona treino/teste
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# Instancia o avaliador autom√°tico (padr√µes)
auto = AutoEnsembleClassifier(
    primary_metric="accuracy",   # m√©trica principal para ranking
    add_rank_columns=True,       # inclui colunas de ranking
    random_state=42,
    n_jobs=-1
)

# Treina e avalia (treina nos dados de treino e avalia no hold-out)
df_holdout = auto.fit_evaluate(X_train, y_train, X_test, y_test, media="macro")

# Resultado: DataFrame com m√©tricas e rankings
print(df_holdout)

# Sum√°rio interpret√°vel
print(auto.summarize_results(top_k=3))
```

### Valida√ß√£o cruzada (CV) ‚Äî estimativa robusta

```python
# Avalia√ß√£o com valida√ß√£o cruzada estratificada
df_cv = auto.fit_evaluate_cv(
    X, y,
    cv=5,
    media="macro",
    random_state_cv=123,
    shuffle=True
)

# df_cv cont√©m <metrica>_mean e <metrica>_std para cada modelo
print(df_cv)
```
 
### Uso avan√ßado ‚Äî sobrescrita de hiperpar√¢metros e salvamento autom√°tico

```python
auto = AutoEnsembleClassifier(
    metricas=["accuracy", "f1", "recall", "precision"],
    primary_metric="f1",
    model_overrides={
        "RandomForest": {"n_estimators": 500, "max_depth": None},
        "ExtraTrees": {"n_estimators": 800},
    },
    output_dir="resultados_bench",
    experiment_name="exp_v1",
    save_on_evaluate=True,
    save_on_cv=True
)

df_holdout = auto.fit_evaluate(X_train, y_train, X_test, y_test, media="macro")
# Arquivos gerados:
# resultados_bench/exp_v1_holdout_results.csv
# resultados_bench/exp_v1_cv_results.csv
```

---

## Explica√ß√£o dos resultados e formato dos arquivos

A biblioteca retorna `pandas.DataFrame` com as colunas descritas abaixo. Caso `save_on_*` esteja ativado, gera CSVs contendo as mesmas informa√ß√µes, mais metadados.

### Colunas de m√©tricas (hold-out)

* `accuracy` ‚Äî fra√ß√£o de previs√µes corretas.
* `f1` ‚Äî pontua√ß√£o F1 (dependente de `media` para problemas multiclasse).
* `recall` ‚Äî sensibilidade (TP / (TP + FN)).
* `precision` ‚Äî precis√£o (TP / (TP + FP)).

> Todas as m√©tricas seguem a API do `scikit-learn`. Use o par√¢metro `media` para ajustar o c√°lculo (`"binary"`, `"macro"`, `"micro"`, `"weighted"`).

### Colunas de ranking

* `rank_accuracy`, `rank_f1`, `rank_recall`, `rank_precision` ‚Äî posi√ß√£o ordinal por m√©trica (1 = melhor).
* `overall_rank` ‚Äî soma das posi√ß√µes das m√©tricas consideradas; menor valor indica melhor desempenho agregado.

**Nota interpretativa:** `overall_rank` √© um agregador de ordens e serve como crit√©rio sumarizador. Ele n√£o substitui testes estat√≠sticos de signific√¢ncia entre modelos.

### Valida√ß√£o cruzada (CV)

Sa√≠da de `fit_evaluate_cv` possui colunas:

* `<metrica>_mean` ‚Äî m√©dia da m√©trica nas folds.
* `<metrica>_std` ‚Äî desvio-padr√£o entre folds (medida de estabilidade).

Use `*_std` para examinar robustez: elevado desvio indica sensibilidade a particionamentos.

### Arquivos gerados

* `{output_dir}/{experiment_name}_holdout_results.csv` ‚Äî resultados do hold-out.
* `{output_dir}/{experiment_name}_cv_results.csv` ‚Äî resultados da valida√ß√£o cruzada.

Os CSVs incluem colunas: `modelo`, m√©tricas, colunas de ranking, par√¢metros aplicados (se sobrescritos), timestamp de execu√ß√£o.

---

## Interpreta√ß√£o metodol√≥gica e recomenda√ß√µes

1. **Sele√ß√£o da m√©trica principal (`primary_metric`)**

   * Para dados desbalanceados, priorize `f1` (ou `recall`/`precision` conforme custo de falsos negativos/positivos).
   * Para problemas multiclasse, utilize `media="macro"` e prefira `f1` como m√©trica agregada.

2. **Hold-out vs CV**

   * Hold-out √© adequado para inspe√ß√£o e diagn√≥stico r√°pidos.
   * Valida√ß√£o cruzada fornece estimativas mais est√°veis e deve ser usada para relat√≥rios cient√≠ficos ou quando o conjunto de dados √© pequeno.

3. **Estabilidade vs desempenho pontual**

   * Compare `*_mean` com `*_std` na CV. Um modelo com m√©dia ligeiramente inferior, mas menor desvio, pode ser prefer√≠vel pela maior robustez.

4. **Compara√ß√µes estat√≠sticas**

   * Ao comparar top-k modelos, realize testes pareados (ex.: Wilcoxon, t-test pareado dependendo da normalidade) sobre as m√©tricas nas folds. Use corre√ß√£o para m√∫ltiplos testes quando necess√°rio.

5. **Reprodutibilidade**

   * Defina `random_state` e `random_state_cv` quando for reportar resultados; versione scripts e CSVs de sa√≠da para rastreabilidade.

---

## API ‚Äî refer√™ncia sucinta

```python
AutoEnsembleClassifier(
    modelos=None,
    metricas=None,
    primary_metric="accuracy",
    add_rank_columns=True,
    random_state=42,
    n_jobs=-1,
    model_overrides=None,
    output_dir=None,
    experiment_name=None,
    save_on_evaluate=False,
    save_on_cv=False,
)
```

### M√©todos principais

* `fit(X_train, y_train)` ‚Äî treina todos os modelos.
* `evaluate(X_test, y_test, media="auto")` ‚Äî avalia modelos no conjunto de teste.
* `fit_evaluate(X_train, y_train, X_test, y_test, media="auto")` ‚Äî atalho: treina e avalia.
* `fit_evaluate_cv(X, y, cv=5, media="auto", random_state_cv=42, shuffle=True)` ‚Äî executa CV estratificada.
* `get_results()` ‚Äî retorna `DataFrame` dos √∫ltimos resultados hold-out.
* `get_results_cv()` ‚Äî retorna `DataFrame` dos √∫ltimos resultados de CV.
* `summarize_results(top_k=3)` ‚Äî resumo textual dos top-k modelos (m√©trica principal).

> Implementa√ß√µes de modelos padr√£o: `RandomForestClassifier`, `ExtraTreesClassifier`, `GradientBoostingClassifier`, `AdaBoostClassifier`, `BaggingClassifier(base_estimator=RandomForest)`.

---

## Licen√ßa

Distribu√≠do sob licen√ßa **MIT** ‚Äî consulte o arquivo `LICENSE` para termos completos.

---

## Sobre o autor

**Vinicius de Souza Santos**
Pesquisador em Ci√™ncia da Computa√ß√£o (UNESP) ‚Äî √™nfase em Machine Learning, Feature Selection e experimenta√ß√£o emp√≠rica.

* **GitHub:** [https://github.com/ViniciusKanh](https://github.com/ViniciusKanh)
* **LinkedIn:** [https://www.linkedin.com/in/vinicius-souza-santoss/](https://www.linkedin.com/in/vinicius-souza-santoss/)
* **E-mail profissional:** [vinicius-souza.santos@unesp.br](mailto:vinicius-souza.santos@unesp.br)

**Resumo das compet√™ncias:** concep√ß√£o e execu√ß√£o de benchmarks reprodut√≠veis, valida√ß√£o cruzada estratificada, experimenta√ß√£o emp√≠rica com scikit-learn, engenharia de pipelines de avalia√ß√£o.

---

### Notas finais

Este projeto foi desenvolvido com a proposta de oferecer uma biblioteca simples, transparente e cient√≠fica para benchmark automatizado de modelos ensemble, focando em reprodutibilidade, rigor estat√≠stico e facilidade de uso para pesquisadores, estudantes e profissionais da √°rea.

